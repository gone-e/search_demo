{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello LTR!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import ltr\n",
    "import ltr.client as client\n",
    "import ltr.index as index\n",
    "import ltr.helpers.movies as helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Elastic client\n",
    "\n",
    "Two LTR clients exist in this code, an ElasticClient and a SolrClient. The workflow for doing Learning to Rank is the same in both search engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch([{'host': 'search-es2.stage.datahou.se', 'port': 443, 'use_ssl': True}])>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = client.ElasticClient()\n",
    "client.elastic_ep\n",
    "client.es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step1. Create FeatureSet\n",
    "\n",
    "실험 버전에 따라서 FeatureSet을 `{myindex}/_ltr/_featureset` 경로에 저장한다.\n",
    "\n",
    "네이밍 규칙은 `{service:card|commerce|...}.{phase:test|dev|stage|prod}.{version:v1|v2}`로 하고\n",
    "버전별로 비교할 수 있는 헬퍼 코드도 마련해놓자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Default LTR feature store [Status: 200]\n",
      "Initialize Default LTR feature store [Status: 200]\n"
     ]
    }
   ],
   "source": [
    "# wipes out any existing LTR models/feature sets in the tmdb index\n",
    "# client.reset_ltr(index='card_search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adding features, we recommend sanity checking that the features work as expected. Adding a “validation” block to your feature creation let’s Elasticsearch LTR run the query before adding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A feature set as a tuple, which looks a lot like JSON\n",
    "feature_set = {\n",
    "    \"validation\": {\n",
    "        \"params\": {\n",
    "            \"query\": \"러그\"\n",
    "        },\n",
    "        \"index\": \"card_search\"\n",
    "    },\n",
    "    \"featureset\": {\n",
    "        \"features\": [\n",
    "            {\n",
    "                \"name\": \"description\",\n",
    "                \"params\": [\n",
    "                    \"query\"\n",
    "                ],\n",
    "                \"template_language\": \"mustache\",\n",
    "                \"template\": {\n",
    "                    \"match\": {\n",
    "                        \"description\": \"{{ query }}\"\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "featureset_name = \"test.card.v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create test.card.v1 feature set [Status: 200]\n"
     ]
    }
   ],
   "source": [
    "# pushes the feature set to the tmdb index's LTR store (a hidden index)\n",
    "# overwrite 가능하다.\n",
    "client.create_featureset(index='card_search', name=featureset_name, ftr_config=feature_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step2. Get Judgment Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RelevanceScore, search_keyword, docid 로 이뤄진 데이터셋(Train/Dev/Test)을 가져온다.\n",
    "\n",
    "```\n",
    "grade,keywords,docId\n",
    "4,rambo,7555\n",
    "3,rambo,1370\n",
    "3,rambo,1369\n",
    "4,rocky,4241\n",
    "```\n",
    "\n",
    "\n",
    "### 2. 설정된 FeatureSet을 선택하여 피쳐를 데이터에 붙여준다. (synthesize)\n",
    "```\n",
    "GET card_search/_search\n",
    "{\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"filter\": [\n",
    "        {\n",
    "          \"terms\": {\n",
    "          \"_id\": [\"11857781\", \"12479875\"]\n",
    "          }\n",
    "        },\n",
    "        {\n",
    "          \"sltr\": {\n",
    "            \"_name\": \"logged_features\",\n",
    "            \"featureset\": \"test.card.v1\",\n",
    "            \"params\": {\"query\": \"러그\"}\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"ext\": {\n",
    "    \"ltr_log\": {\n",
    "      \"log_specs\": {\n",
    "        \"name\": \"ltr_features\",\n",
    "        \"named_query\": \"logged_features\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"size\": 1000\n",
    "}\n",
    "```\n",
    "\n",
    "```\n",
    "1   qid:1   1:1998.0 # 4518 \n",
    "0   qid:1   1:2016.0 # 375315   \n",
    "1   qid:1   1:2005.0 # 16608    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': {'bool': {'filter': [{'sltr': {'_name': 'logged_features', 'featureset': 'test.card.v1', 'params': {'query': '러그'}}}], 'must': [{'terms': {'_id': ['11857781', '12479875']}}]}}, 'ext': {'ltr_log': {'log_specs': {'name': 'ltr_features', 'named_query': 'logged_features'}}}, 'size': 1000}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def synthesize(\n",
    "    client,\n",
    "    featureset_name,\n",
    "    TrainingSetOut='test.train.txt',\n",
    "):\n",
    "    from ltr.judgments import judgments_to_file, Judgment\n",
    "    NO_ZERO = False\n",
    "\n",
    "    # resp = client.log_query('tmdb', 'release', None)\n",
    "    params, resp = client.log_query('card_search', featureset_name, ids=[\"11857781\", \"12479875\"], params={\"query\": \"러그\"})\n",
    "    print(params)\n",
    "\n",
    "    # A classic film fan\n",
    "    judgments = []\n",
    "\n",
    "    for hit in resp:\n",
    "        judgments.append(Judgment(\n",
    "            qid=1,\n",
    "            docId=hit['id'],\n",
    "            grade=random.choice([0,1,2,3,4]),\n",
    "            features=hit['ltr_features'],\n",
    "            keywords=''\n",
    "            )\n",
    "        )\n",
    "\n",
    "    with open(TrainingSetOut, 'w') as out:\n",
    "        judgments_to_file(out, judgments)\n",
    "\n",
    "synthesize(client, featureset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/classic-training.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/md/k_545kt57fl0mnpkzrkzv42hjp98mh/T/ipykernel_94631/748452902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mltr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjudgments\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjudge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mclassic_training_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjudge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjudgments_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/classic-training.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlatest_training_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjudge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjudgments_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/latest-training.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclassic_training_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/classic-training.txt'"
     ]
    }
   ],
   "source": [
    "import ltr.judgments as judge\n",
    "\n",
    "classic_training_set = [j for j in judge.judgments_from_file(open('data/classic-training.txt'))]\n",
    "latest_training_set = [j for j in judge.judgments_from_file(open('data/latest-training.txt'))]\n",
    "classic_training_set[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3. Learning LTR Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Submit\n",
    "\n",
    "We'll train a lot of models in this class! Our ltr library has a `train` method that wraps a tool called `Ranklib` (more on Ranklib later), allows you to pass the most common commands to Ranklib, stores a model in the search engine, and then returns diagnostic output that's worth inspecting. \n",
    "\n",
    "For now we'll just train using the generated training set, and store two models `latest` and `classic`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltr.ranklib import train\n",
    "\n",
    "train(client, training_set=latest_training_set, \n",
    "      index='tmdb', featureSet='release', modelName='latest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train another model based on the 'classsic' movie judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(client, training_set=classic_training_set, \n",
    "      index='tmdb', featureSet='release', modelName='classic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4. Upload LTR Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step5. Predict (Run `sltr` query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ben Affleck vs Adam West\n",
    "If we search for `batman`, how do the results compare?  Since the `classic` model prefered old movies it has old movies in the top position, and the opposite is true for the `latest` model.  To continue learning LTR, brainstorm more features and generate some real judgments for real queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ltr.release_date_plot as rdp\n",
    "rdp.plot(client, 'batman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See top 12 results for both models\n",
    "\n",
    "Looking at the `classic` model first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "classic_results = rdp.search(client, 'batman', 'classic')\n",
    "pd.json_normalize(classic_results)[['id', 'title', 'release_year', 'score']].head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the `latest` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_results = rdp.search(client, 'batman', 'latest')\n",
    "pd.json_normalize(latest_results)[['id', 'title', 'release_year', 'score']].head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
